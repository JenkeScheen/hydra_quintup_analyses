{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation of t-SNE overlap approach\n",
    "Make t-SNE of all freesolv quintuplicates. Make a split such that the test quints are not well-represented in the training set. Check if these are poorly predicted on by a model. Then incrementally add back the most un-represented cases and update statistics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"/home/jscheen/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/home/jscheen/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/home/jscheen/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"/usr/lib/python3.6/imp.py\", line 243, in load_module\n    return load_dynamic(name, filename, file)\n  File \"/usr/lib/python3.6/imp.py\", line 343, in load_dynamic\n    return _load(spec)\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_mod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0m_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_pywrap_tensorflow_internal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/imp.py\u001b[0m in \u001b[0;36mload_module\u001b[0;34m(name, file, filename, details)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_dynamic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtype_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPKG_DIRECTORY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/imp.py\u001b[0m in \u001b[0;36mload_dynamic\u001b[0;34m(name, path, file)\u001b[0m\n\u001b[1;32m    342\u001b[0m             name=name, loader=loader, origin=path)\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: libcublas.so.9.0: cannot open shared object file: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5b3bd9a0515a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_DEVICE_ORDER\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"PCI_BUS_ID\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m  \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcomponent_api_helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[0;32m---> 74\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"/home/jscheen/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/home/jscheen/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/home/jscheen/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"/usr/lib/python3.6/imp.py\", line 243, in load_module\n    return load_dynamic(name, filename, file)\n  File \"/usr/lib/python3.6/imp.py\", line 343, in load_dynamic\n    return _load(spec)\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import csv\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "plot_kwds = {'alpha' : 0.5, 's' : 80, 'linewidths':0}\n",
    "\n",
    "import sklearn\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy import stats\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import Draw, rdFMCS, AllChem, rdmolfiles, Descriptors, rdchem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "quints_fps = pd.read_csv(\"output/quints_fps.csv\", header=None)\n",
    "quints_infos = pd.read_csv(\"output/quints_infos.csv\", names=[\"set\", \"pertname\", \"pertsmarts\", \"num_ha\", \"sem\"])\n",
    "\n",
    "quints_whole_df = pd.concat([quints_infos, quints_fps], axis=1)\n",
    "\n",
    "# drop NaN columns (happens with molprop generation where (error) strings can't be subtracted)\n",
    "quints_whole_df = quints_whole_df.dropna(axis=1)\n",
    "\n",
    "# drop rows where SEM == 0.0. It seems some very large perturbations get this value too, so makes training noisy.\n",
    "quints_whole_df = quints_whole_df[quints_whole_df[\"sem\"] > 0.0001]\n",
    "\n",
    "# drop columns where all values are 0.\n",
    "quints_whole_df = quints_whole_df.loc[:, (quints_whole_df != 0).any(axis=0)]\n",
    "\n",
    "# TMP DROP DUPLICATES --> should be fewer duplicates when we move up to more features.\n",
    "quints_whole_df = quints_whole_df.drop_duplicates(subset=quints_whole_df.columns.difference(['sem','set','pertname','pertsmarts']))\n",
    "\n",
    "quints_whole_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretize SEM label into categorical bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### TMP DISCRETIZE BY STRATIFICATION:\n",
    "n_bins=20\n",
    "\n",
    "binned_sem = pd.qcut(quints_whole_df[\"sem\"], n_bins, labels=False)\n",
    "quints_whole_df[\"sem_bin\"] = binned_sem\n",
    "print(\"Bin, Min, Max, Volume\")\n",
    "for n_bin, df_group in quints_whole_df.groupby(by=\"sem_bin\"):\n",
    "    print(n_bin, round(min(df_group[\"sem\"].values), 2), round(max(df_group[\"sem\"].values), 2), len(df_group))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### TMP SLIM DOWN DOMINANT CLASSES\n",
    "# quints_whole_df.sort_values(by=\"sem_bin\")\n",
    "# quints_whole_df = quints_whole_df.groupby(\"sem_bin\").head(100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quints_fps = quints_whole_df.drop([\"set\", \"pertname\", \"pertsmarts\", \"num_ha\", \"sem\", \"sem_bin\"], axis=1)\n",
    "quints_fps = quints_fps.values\n",
    "quints_infos = quints_whole_df[[\"set\", \"pertname\", \"pertsmarts\", \"num_ha\", \"sem\", \"sem_bin\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitTSNE(fps):\n",
    "\n",
    "    # https://iwatobipen.wordpress.com/2019/12/22/clustering-molecules-with-hdbscan-and-predict-chemical-space-with-scikit-learn-chemoinformatics-rdkit/\n",
    "    tsne = TSNE(random_state=42)\n",
    "\n",
    "    res = tsne.fit_transform(fps)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = fitTSNE(quints_fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "upper_test_indices = []\n",
    "lower_test_indices = []\n",
    "train_indices = []\n",
    "\n",
    "colours = []\n",
    "\n",
    "for i, (x, y) in enumerate(zip(res[:,0], res[:,1])):\n",
    "    if 30 < x < 40 and -10 < y < 5:\n",
    "        upper_test_indices.append(i)\n",
    "        colours.append(\"Test set\")\n",
    "    elif -10 < x < -2 and 25 < y < 35:\n",
    "        lower_test_indices.append(i)\n",
    "        colours.append(\"Test set\")\n",
    "    else:\n",
    "        train_indices.append(i)\n",
    "        colours.append(\"Training set\")\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(x=res[:,0], y=res[:,1], hue=colours, **plot_kwds)\n",
    "\n",
    "plt.xlabel(\"t-SNE 0\")\n",
    "plt.ylabel(\"t-SNE 1\")\n",
    "plt.title(\"tSNE plot of simulated FreeSolv quintuplicates\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train (orange) and test (blue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def takeSubset(indices, quints_whole_df):\n",
    "    \"\"\"Take a selection of a dataframe using indices\"\"\"\n",
    "    subset = quints_whole_df.iloc[indices]\n",
    "    \n",
    "    return subset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def takeInfo(perts_df):\n",
    "    \"\"\"from the input dataframe, return arrays of fingerprints and SEMs\"\"\"\n",
    "    sems = perts_df[\"sem_bin\"].values\n",
    "    \n",
    "    # fps is a bit more involved. Remove everything but the FP columns, return as 2d array.\n",
    "    fps_df = perts_df.drop([\"set\", \"pertname\", \"pertsmarts\", \"num_ha\", \"sem\", \"sem_bin\"], axis=1)\n",
    "    fps = fps_df.values\n",
    "\n",
    "    return fps, sems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessSets(test1, test2, train):\n",
    "    \"\"\"standardises and reduces dimensionality to 95% VE; returns arrays\"\"\"\n",
    "    \n",
    "    #### fit the scaler on training set.\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    train_scaled = scaler.fit_transform(train)\n",
    "    \n",
    "    # transform both the training set and the test sets.\n",
    "    test1_scaled = scaler.transform(test1)\n",
    "    test2_scaled = scaler.transform(test2)\n",
    "    \n",
    "    #### fit PCA on training set with 95% variance explained.\n",
    "    pca = PCA(n_components=0.95)\n",
    "    train_preprocessed = pca.fit_transform(train_scaled)\n",
    "    \n",
    "    # transform both the training set and the test sets.\n",
    "    test1_preprocessed = pca.transform(test1_scaled)\n",
    "    test2_preprocessed = pca.transform(test2_scaled)\n",
    "    \n",
    "    return test1_preprocessed, test2_preprocessed, train_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get fps, sem, preprocess (i.e. scale and reduce dimensionality)\n",
    "# upper_test_fps, upper_test_sems = takeInfo(takeSubset(upper_test_indices, quints_whole_df))\n",
    "# lower_test_fps, lower_test_sems = takeInfo(takeSubset(lower_test_indices, quints_whole_df))\n",
    "# train_fps, train_sems = takeInfo(takeSubset(train_indices, quints_whole_df))\n",
    "\n",
    "# test_set1, test_set2, train_set = preProcessSets(upper_test_fps, lower_test_fps, train_fps)\n",
    "\n",
    "# n_classes = len(set(train_sems))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TMP RANDOM SPLIT INSTEAD OF TSNE SPLIT\n",
    "from sklearn.model_selection import train_test_split\n",
    "whole_set, whole_set_sem_bins = takeInfo(takeSubset(np.array(range(len(quints_whole_df))), quints_whole_df))\n",
    "\n",
    "train_set, test_set1, train_sems, upper_test_sems = train_test_split(whole_set, whole_set_sem_bins, test_size=0.3, random_state=42)\n",
    "n_classes = len(set(train_sems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def build_model(loss='sparse_categorical_crossentropy', \n",
    "                lr=1e-3, \n",
    "                num_neurons_1=10,\n",
    "                num_neurons_2=None,\n",
    "                num_neurons_3=None,\n",
    "                num_neurons_4=None,\n",
    "                input_shape=train_set.shape[1],\n",
    "                act=\"relu\",\n",
    "                optimizer=\"adam\"):\n",
    "    # standard setup FF DNN with input shape of input data.\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape))\n",
    "    \n",
    "    model.add(keras.layers.Dropout(0.4))\n",
    "    # first layer.\n",
    "    model.add(keras.layers.Dense(num_neurons_1, activation=act))\n",
    "    model.add(keras.layers.Dropout(0.4))\n",
    "    \n",
    "    # second layer.\n",
    "    if num_neurons_2:\n",
    "        model.add(keras.layers.Dense(num_neurons_2, activation=act))\n",
    "        model.add(keras.layers.Dropout(0.4))\n",
    "    \n",
    "    # third layer.\n",
    "    if num_neurons_3:\n",
    "        model.add(keras.layers.Dense(num_neurons_3, activation=act))\n",
    "        model.add(keras.layers.Dropout(0.4))\n",
    "        \n",
    "    # fourth layer.\n",
    "    if num_neurons_4:\n",
    "        model.add(keras.layers.Dense(num_neurons_4, activation=act))\n",
    "        model.add(keras.layers.Dropout(0.4))\n",
    "    \n",
    "    # standard last linear layer for regression.\n",
    "    #model.add(keras.layers.Dense(1, activation=\"linear\"))\n",
    "    \n",
    "    # standard last layer for classification.\n",
    "    model.add(keras.layers.Dense(n_classes, activation=\"softmax\"))\n",
    "    \n",
    "\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "keras_reg = keras.wrappers.scikit_learn.KerasClassifier(build_model, epochs=500, verbose=0)\n",
    "\n",
    "params_distrib = {\n",
    "    \"loss\": ['sparse_categorical_crossentropy', 'poisson'],\n",
    "    \"lr\": [1e-1, 1e-2, 1e-3, 1e-4, 1e-5],\n",
    "    \"num_neurons_1\": [5, 10, 20, 40],\n",
    "    \"num_neurons_2\": [None, 8, 20, 40, 80],\n",
    "    \"num_neurons_3\": [None, 8, 20, 40, 80],\n",
    "    \"num_neurons_4\": [None, 8, 20, 40, 80],\n",
    "    \"act\": [\"relu\", \"tanh\", \"sigmoid\"],\n",
    "    \"optimizer\": [\"adam\", \"SGD\"]\n",
    "}\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, params_distrib, n_iter=20, n_jobs=5, cv=5, verbose=2)\n",
    "rnd_result = rnd_search_cv.fit(np.array(train_set), np.array(train_sems), \n",
    "                               ) \n",
    "print(rnd_result.best_params_)\n",
    "print(rnd_result.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# see ALL validation scores.\n",
    "rnd_result.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the indicated hyperparameters to overfit a single DNN, check loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_opt(loss, \n",
    "                lr, \n",
    "                num_neurons_1,\n",
    "                num_neurons_2,\n",
    "                num_neurons_3,\n",
    "                num_neurons_4,\n",
    "                act,\n",
    "                optimizer,\n",
    "                input_shape=train_set.shape[1]):\n",
    "    # standard setup FF DNN with input shape of input data.\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dropout(0.4, input_shape=(input_shape,)))\n",
    "    \n",
    "    # first layer.\n",
    "    model.add(keras.layers.Dense(num_neurons_1, activation=act))\n",
    "    model.add(keras.layers.Dropout(0.4))\n",
    "    \n",
    "    # second layer.\n",
    "    if num_neurons_2:\n",
    "        model.add(keras.layers.Dense(num_neurons_2, activation=act))\n",
    "        model.add(keras.layers.Dropout(0.4))\n",
    "    \n",
    "    # third layer.\n",
    "    if num_neurons_3:\n",
    "        model.add(keras.layers.Dense(num_neurons_3, activation=act))\n",
    "        model.add(keras.layers.Dropout(0.4))\n",
    "        \n",
    "    # fourth layer.\n",
    "    if num_neurons_4:\n",
    "        model.add(keras.layers.Dense(num_neurons_4, activation=act))\n",
    "        model.add(keras.layers.Dropout(0.4))\n",
    "    \n",
    "    # standard last linear layer for regression.\n",
    "    #model.add(keras.layers.Dense(1, activation=\"linear\"))\n",
    "    \n",
    "    # standard last layer for classification.\n",
    "    model.add(keras.layers.Dense(n_classes, activation=\"softmax\"))\n",
    "    optimizer = keras.optimizers.Adam(lr=0.0001)\n",
    "\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=50)\n",
    "dnn_model = build_model_opt(**rnd_result.best_params_)\n",
    "history = dnn_model.fit(\n",
    "    np.array(train_set), np.array(train_sems),\n",
    "    validation_split=0.2,\n",
    "    verbose=0, batch_size=64, epochs=5000,\n",
    "    #callbacks=[callback]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history, lim):\n",
    "    plt.plot(history.history['accuracy'], label='accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    if lim:\n",
    "        plt.xlim(lim)\n",
    "    #plt.ylim(0,1)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history, [0, 5000])\n",
    "\n",
    "#plot_train_scatter(dnn_model, train_set, train_sems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for pred in dnn_model.predict(test_set1):\n",
    "    print(np.argmax(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "from sklearn.svm import SVC \n",
    "\n",
    "model = SVC()\n",
    "\n",
    "\n",
    "param_grid = {'C': [0.1, 1, 10, 100, 1000],  \n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "              'kernel': ['rbf']}  \n",
    "  \n",
    "grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 1) \n",
    "  \n",
    "# fitting the model for grid search \n",
    "grid.fit(np.array(train_set), np.array(train_sems)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print best parameter after tuning \n",
    "print(grid.best_params_) \n",
    "  \n",
    "# print how our model looks after hyper-parameter tuning \n",
    "print(grid.best_estimator_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_predictions = grid.predict(test_set1) \n",
    "  \n",
    "# print classification report \n",
    "print(classification_report(upper_test_sems, grid_predictions)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(random_state = 1)\n",
    "\n",
    "n_estimators = [100, 300, 500, 800, 1200]\n",
    "max_depth = [5, 8, 15, 25, 30, 50]\n",
    "min_samples_split = [2, 5, 10, 15, 100]\n",
    "min_samples_leaf = [1, 2, 5, 10] \n",
    "\n",
    "hyperF = dict(n_estimators = n_estimators, max_depth = max_depth,  \n",
    "              min_samples_split = min_samples_split, \n",
    "             min_samples_leaf = min_samples_leaf)\n",
    "\n",
    "gridF = GridSearchCV(forest, hyperF, cv = 5, verbose = 1, \n",
    "                      n_jobs = -1)\n",
    "bestF = gridF.fit(np.array(train_set), np.array(train_sems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print best parameter after tuning \n",
    "print(gridF.best_params_) \n",
    "  \n",
    "# print how our model looks after hyper-parameter tuning \n",
    "print(gridF.best_estimator_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_predictions = gridF.predict(test_set1) \n",
    "  \n",
    "# print classification report \n",
    "print(classification_report(upper_test_sems, grid_predictions)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
